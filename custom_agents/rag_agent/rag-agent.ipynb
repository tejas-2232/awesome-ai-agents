{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccafa88c",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) AI Agent\n",
    "\n",
    "This notebook demonstrates how to create a RAG-based AI agent that can retrieve information from external knowledge sources and generate informed responses.\n",
    "\n",
    "#### Features:\n",
    "* Document ingestion and vector storage\n",
    "* Semantic search and retrieval\n",
    "* Context-aware response generation\n",
    "* Multiple document formats support\n",
    "* Relevance scoring and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c40a69",
   "metadata": {},
   "source": [
    "#### Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai tiktoken numpy scikit-learn sentence-transformers python-dotenv PyPDF2 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ff950",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb224f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5b8fa",
   "metadata": {},
   "source": [
    "#### Document and Chunk Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223169dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Represents a document in the knowledge base\n",
    "    \n",
    "    \"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    source: str\n",
    "    created_at: datetime = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.created_at is None:\n",
    "            self.created_at = datetime.now()\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Represents a chunk of a document for retrieval\"\"\"\n",
    "    id: str\n",
    "    document_id: str\n",
    "    content: str\n",
    "    embedding: Optional[np.ndarray]\n",
    "    metadata: Dict[str, Any]\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "    \n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Represents a retrieval result with relevance score\"\"\"\n",
    "    chunk: DocumentChunk\n",
    "    score: float\n",
    "    document_title: str\n",
    "    document_source: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c30cb3",
   "metadata": {},
   "source": [
    "#### Text Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"Handles text processing and chunking for documents\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 encoding\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()\\[\\]-]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def split_text_by_tokens(self, text: str, max_tokens: int = None) -> List[str]:\n",
    "        \"\"\"Split text into chunks based on token count\"\"\"\n",
    "        if max_tokens is None:\n",
    "            max_tokens = self.chunk_size\n",
    "        \n",
    "        # Split by sentences first\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If adding this sentence would exceed token limit\n",
    "            if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = self._get_overlap_text(current_chunk, self.chunk_overlap)\n",
    "                current_chunk = overlap_text + sentence\n",
    "                current_tokens = self.count_tokens(current_chunk)\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _get_overlap_text(self, text: str, overlap_tokens: int) -> str:\n",
    "        \"\"\"Get the last portion of text for overlap\"\"\"\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return \"\"\n",
    "        \n",
    "        # Estimate words needed for overlap (rough approximation)\n",
    "        overlap_words = overlap_tokens // 4  # Assume ~4 tokens per word\n",
    "        overlap_words = min(overlap_words, len(words))\n",
    "        \n",
    "        return \" \".join(words[-overlap_words:]) + \" \"\n",
    "    \n",
    "    def create_chunks(self, document: Document) -> List[DocumentChunk]:\n",
    "        \"\"\"Create chunks from a document\"\"\"\n",
    "        cleaned_text = self.clean_text(document.content)\n",
    "        text_chunks = self.split_text_by_tokens(cleaned_text)\n",
    "        \n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            chunk_id = f\"{document.id}_chunk_{i}\"\n",
    "            end_index = start_index + len(chunk_text)\n",
    "            \n",
    "            chunk = DocumentChunk(\n",
    "                id=chunk_id,\n",
    "                document_id=document.id,\n",
    "                content=chunk_text,\n",
    "                embedding=None,  # Will be set later\n",
    "                metadata={\n",
    "                    **document.metadata,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"token_count\": self.count_tokens(chunk_text)\n",
    "                },\n",
    "                start_index=start_index,\n",
    "                end_index=end_index\n",
    "            )\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            start_index = end_index\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b72f85",
   "metadata": {},
   "source": [
    "#### Vector Store for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400704d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Simple in-memory vector store for document embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.chunks: Dict[str, DocumentChunk] = {}\n",
    "        self.documents: Dict[str, Document] = {}\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.chunk_ids: List[str] = []\n",
    "    \n",
    "    def add_document(self, document: Document, chunks: List[DocumentChunk]):\n",
    "        \"\"\"Add a document and its chunks to the vector store\"\"\"\n",
    "        self.documents[document.id] = document\n",
    "        \n",
    "        # Generate embeddings for chunks\n",
    "        chunk_texts = [chunk.content for chunk in chunks]\n",
    "        chunk_embeddings = self.embedding_model.encode(chunk_texts)\n",
    "        \n",
    "        # Store chunks with embeddings\n",
    "        for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "            chunk.embedding = embedding\n",
    "            self.chunks[chunk.id] = chunk\n",
    "        \n",
    "        # Update the embedding matrix\n",
    "        self._rebuild_embedding_matrix()\n",
    "    \n",
    "    def _rebuild_embedding_matrix(self):\n",
    "        \"\"\"Rebuild the embedding matrix for efficient similarity search\"\"\"\n",
    "        if not self.chunks:\n",
    "            self.embeddings = None\n",
    "            self.chunk_ids = []\n",
    "            return\n",
    "        \n",
    "        self.chunk_ids = list(self.chunks.keys())\n",
    "        embeddings_list = [self.chunks[chunk_id].embedding for chunk_id in self.chunk_ids]\n",
    "        self.embeddings = np.vstack(embeddings_list)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5, min_score: float = 0.0) -> List[RetrievalResult]:\n",
    "        \"\"\"Search for relevant chunks based on query\"\"\"\n",
    "        if not self.chunks or self.embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            score = similarities[idx]\n",
    "            if score < min_score:\n",
    "                break\n",
    "                \n",
    "            chunk_id = self.chunk_ids[idx]\n",
    "            chunk = self.chunks[chunk_id]\n",
    "            document = self.documents[chunk.document_id]\n",
    "            \n",
    "            result = RetrievalResult(\n",
    "                chunk=chunk,\n",
    "                score=score,\n",
    "                document_title=document.title,\n",
    "                document_source=document.source\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_document_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the vector store\"\"\"\n",
    "        return {\n",
    "            \"total_documents\": len(self.documents),\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"embedding_dimension\": self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            \"model_name\": self.embedding_model.get_sentence_embedding_dimension()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8375fbd",
   "metadata": {},
   "source": [
    "#### RAG Agent Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf71b88",
   "metadata": {},
   "source": [
    "class RAGAgent:\n",
    "    \"\"\"Retrieval-Augmented Generation AI Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", max_context_tokens: int = 3000):\n",
    "        self.model = model\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.text_processor = TextProcessor()\n",
    "        self.vector_store = VectorStore()\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a knowledgeable AI assistant that uses retrieved information to provide accurate, \n",
    "        well-informed responses. When answering questions:\n",
    "        \n",
    "        1. Use the provided context information to support your answers\n",
    "        2. Clearly distinguish between information from the context and your general knowledge\n",
    "        3. If the context doesn't contain enough information, say so honestly\n",
    "        4. Cite sources when possible\n",
    "        5. Provide comprehensive but concise answers\n",
    "        \n",
    "        Always prioritize accuracy and helpfulness in your responses.\n",
    "        \"\"\"\n",
    "    \n",
    "    def add_document_from_text(self, title: str, content: str, source: str = \"manual\", metadata: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Add a document from text content\"\"\"\n",
    "        doc_id = f\"doc_{len(self.vector_store.documents)}_{hash(title) % 10000}\"\n",
    "        \n",
    "        document = Document(\n",
    "            id=doc_id,\n",
    "            title=title,\n",
    "            content=content,\n",
    "            metadata=metadata or {},\n",
    "            source=source\n",
    "        )\n",
    "        \n",
    "        chunks = self.text_processor.create_chunks(document)\n",
    "        self.vector_store.add_document(document, chunks)\n",
    "        \n",
    "        return doc_id\n",
    "    \n",
    "    def add_document_from_url(self, url: str, title: str = None) -> str:\n",
    "        \"\"\"Add a document by fetching content from a URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.text\n",
    "            # Basic HTML cleaning (you might want to use BeautifulSoup for better results)\n",
    "            content = re.sub(r'<[^>]+>', '', content)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            \n",
    "            if not title:\n",
    "                title = f\"Web content from {url}\"\n",
    "            \n",
    "            return self.add_document_from_text(title, content, source=url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to fetch content from {url}: {str(e)}\")\n",
    "    \n",
    "    def retrieve_context(self, query: str, top_k: int = 3, min_score: float = 0.1) -> Tuple[str, List[RetrievalResult]]:\n",
    "        \"\"\"Retrieve relevant context for a query\"\"\"\n",
    "        results = self.vector_store.search(query, top_k=top_k, min_score=min_score)\n",
    "        \n",
    "        if not results:\n",
    "            return \"\", []\n",
    "        \n",
    "        # Build context string\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            context_parts.append(\n",
    "                f\"[Source {i}: {result.document_title} (Score: {result.score:.3f})]\\n\"\n",
    "                f\"{result.chunk.content}\\n\"\n",
    "            )\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Ensure context doesn't exceed token limit\n",
    "        if self.text_processor.count_tokens(context) > self.max_context_tokens:\n",
    "            # Truncate context to fit within limits\n",
    "            words = context.split()\n",
    "            truncated_words = words[:self.max_context_tokens]\n",
    "            context = \" \".join(truncated_words)\n",
    "        \n",
    "        return context, results\n",
    "    \n",
    "    def generate_response(self, query: str, use_conversation_history: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a response using RAG\"\"\"\n",
    "        # Retrieve relevant context\n",
    "        context, retrieval_results = self.retrieve_context(query)\n",
    "        \n",
    "        # Build messages for the API call\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "        # Add conversation history if requested\n",
    "        if use_conversation_history and self.conversation_history:\n",
    "            messages.extend(self.conversation_history[-6:])  # Last 3 exchanges\n",
    "        \n",
    "        # Add context and query\n",
    "        if context:\n",
    "            user_message = f\"\"\"\n",
    "            Context Information:\n",
    "            {context}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Please answer the question using the provided context information. \n",
    "            If the context doesn't contain sufficient information, please indicate that.\n",
    "            \"\"\"\n",
    "        else:\n",
    "            user_message = f\"\"\"\n",
    "            Question: {query}\n",
    "            \n",
    "            Note: No relevant context was found in the knowledge base. \n",
    "            Please answer based on your general knowledge but indicate that \n",
    "            this is not from the provided documents.\n",
    "            \"\"\"\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                max_tokens=500,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            # Update conversation history\n",
    "            if use_conversation_history:\n",
    "                self.conversation_history.extend([\n",
    "                    {\"role\": \"user\", \"content\": query},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "                ])\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": assistant_response,\n",
    "                \"context_used\": context,\n",
    "                \"retrieval_results\": retrieval_results,\n",
    "                \"sources_count\": len(retrieval_results)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"context_used\": context,\n",
    "                \"retrieval_results\": retrieval_results\n",
    "            }\n",
    "    \n",
    "    def clear_conversation_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def get_knowledge_base_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the knowledge base\"\"\"\n",
    "        stats = self.vector_store.get_document_stats()\n",
    "        stats[\"text_processor_settings\"] = {\n",
    "            \"chunk_size\": self.text_processor.chunk_size,\n",
    "            \"chunk_overlap\": self.text_processor.chunk_overlap\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def list_documents(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all documents in the knowledge base\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"id\": doc.id,\n",
    "                \"title\": doc.title,\n",
    "                \"source\": doc.source,\n",
    "                \"created_at\": doc.created_at.isoformat(),\n",
    "                \"content_length\": len(doc.content),\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            for doc in self.vector_store.documents.values()\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf22dc",
   "metadata": {},
   "source": [
    "#### Example Usage and Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG agent\n",
    "rag_agent = RAGAgent()\n",
    "\n",
    "print(\"=== RAG Agent Initialized ===\")\n",
    "print(f\"Model: {rag_agent.model}\")\n",
    "print(f\"Max context tokens: {rag_agent.max_context_tokens}\")\n",
    "print(f\"Chunk size: {rag_agent.text_processor.chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd68f7f",
   "metadata": {},
   "source": [
    "__Adding Sample Documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5397b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sample documents to demonstrate RAG capabilities\n",
    "\n",
    "# Document 1: AI and Machine Learning overview\n",
    "ai_content = \"\"\"\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines \n",
    "that can perform tasks that typically require human intelligence. These tasks include learning, \n",
    "reasoning, problem-solving, perception, and language understanding.\n",
    "\n",
    "Machine Learning (ML) is a subset of AI that focuses on the development of algorithms and statistical \n",
    "models that enable computers to improve their performance on a specific task through experience, \n",
    "without being explicitly programmed.\n",
    "\n",
    "Deep Learning is a subset of machine learning that uses artificial neural networks with multiple \n",
    "layers (hence \"deep\") to model and understand complex patterns in data. It has been particularly \n",
    "successful in areas such as image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "Common applications of AI include:\n",
    "- Computer vision and image recognition\n",
    "- Natural language processing and chatbots\n",
    "- Recommendation systems\n",
    "- Autonomous vehicles\n",
    "- Predictive analytics\n",
    "- Game playing (like chess and Go)\n",
    "\n",
    "The field of AI has experienced rapid growth in recent years, driven by advances in computing power, \n",
    "the availability of large datasets, and improvements in algorithms.\n",
    "\"\"\"\n",
    "\n",
    "doc1_id = rag_agent.add_document_from_text(\n",
    "    title=\"Introduction to Artificial Intelligence and Machine Learning\",\n",
    "    content=ai_content,\n",
    "    source=\"AI Tutorial\",\n",
    "    metadata={\"category\": \"education\", \"topic\": \"AI/ML\"}\n",
    ")\n",
    "\n",
    "# Document 2: Python programming guide\n",
    "python_content = \"\"\"\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
    "It was created by Guido van Rossum and first released in 1991.\n",
    "\n",
    "Key features of Python:\n",
    "- Easy to learn and use syntax\n",
    "- Interpreted language (no compilation needed)\n",
    "- Object-oriented and functional programming support\n",
    "- Extensive standard library\n",
    "- Large ecosystem of third-party packages\n",
    "- Cross-platform compatibility\n",
    "\n",
    "Python is widely used in various domains:\n",
    "- Web development (Django, Flask)\n",
    "- Data science and analytics (Pandas, NumPy, Matplotlib)\n",
    "- Machine learning (Scikit-learn, TensorFlow, PyTorch)\n",
    "- Automation and scripting\n",
    "- Scientific computing\n",
    "- Game development\n",
    "\n",
    "Popular Python libraries:\n",
    "- NumPy: Numerical computing\n",
    "- Pandas: Data manipulation and analysis\n",
    "- Matplotlib: Data visualization\n",
    "- Requests: HTTP library\n",
    "- Django: Web framework\n",
    "- Flask: Lightweight web framework\n",
    "\n",
    "Python's philosophy emphasizes code readability and simplicity, following the principle that \n",
    "\"there should be one obvious way to do it.\"\n",
    "\"\"\"\n",
    "\n",
    "doc2_id = rag_agent.add_document_from_text(\n",
    "    title=\"Python Programming Language Guide\",\n",
    "    content=python_content,\n",
    "    source=\"Programming Tutorial\",\n",
    "    metadata={\"category\": \"programming\", \"language\": \"python\"}\n",
    ")\n",
    "\n",
    "# Document 3: Data Science overview\n",
    "data_science_content = \"\"\"\n",
    "Data Science is an interdisciplinary field that combines statistics, computer science, and domain \n",
    "expertise to extract insights and knowledge from data. It involves collecting, processing, analyzing, \n",
    "and interpreting large amounts of data to inform decision-making.\n",
    "\n",
    "The data science process typically includes:\n",
    "1. Problem definition and goal setting\n",
    "2. Data collection and acquisition\n",
    "3. Data cleaning and preprocessing\n",
    "4. Exploratory data analysis (EDA)\n",
    "5. Feature engineering and selection\n",
    "6. Model building and training\n",
    "7. Model evaluation and validation\n",
    "8. Deployment and monitoring\n",
    "\n",
    "Key skills for data scientists:\n",
    "- Programming (Python, R, SQL)\n",
    "- Statistics and probability\n",
    "- Machine learning algorithms\n",
    "- Data visualization\n",
    "- Domain knowledge\n",
    "- Communication skills\n",
    "\n",
    "Common tools and technologies:\n",
    "- Python libraries: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn\n",
    "- R and RStudio\n",
    "- SQL databases\n",
    "- Jupyter Notebooks\n",
    "- Tableau, Power BI for visualization\n",
    "- Apache Spark for big data processing\n",
    "- Cloud platforms: AWS, Google Cloud, Azure\n",
    "\n",
    "Data science applications span many industries including healthcare, finance, retail, technology, \n",
    "and government, helping organizations make data-driven decisions.\n",
    "\"\"\"\n",
    "\n",
    "doc3_id = rag_agent.add_document_from_text(\n",
    "    title=\"Data Science: Process, Skills, and Tools\",\n",
    "    content=data_science_content,\n",
    "    source=\"Data Science Guide\",\n",
    "    metadata={\"category\": \"data science\", \"topic\": \"overview\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Documents Added ===\")\n",
    "print(f\"Document 1 ID: {doc1_id}\")\n",
    "print(f\"Document 2 ID: {doc2_id}\")\n",
    "print(f\"Document 3 ID: {doc3_id}\")\n",
    "\n",
    "# Show knowledge base stats\n",
    "stats = rag_agent.get_knowledge_base_stats()\n",
    "print(f\"\\nKnowledge Base Stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b9882",
   "metadata": {},
   "source": [
    "Example 1: Basic RAG Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e672888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Example 1: Basic RAG Queries ===\")\n",
    "\n",
    "# Query 1: About AI and ML\n",
    "query1 = \"What is machine learning and how does it relate to AI?\"\n",
    "result1 = rag_agent.generate_response(query1)\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(f\"Response: {result1.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {result1.get('sources_count', 0)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Query 2: About Python\n",
    "query2 = \"What are the key features of Python programming language?\"\n",
    "result2 = rag_agent.generate_response(query2)\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(f\"Response: {result2.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {result2.get('sources_count', 0)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Query 3: About Data Science\n",
    "query3 = \"What skills do I need to become a data scientist?\"\n",
    "result3 = rag_agent.generate_response(query3)\n",
    "\n",
    "print(f\"Query: {query3}\")\n",
    "print(f\"Response: {result3.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {result3.get('sources_count', 0)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
