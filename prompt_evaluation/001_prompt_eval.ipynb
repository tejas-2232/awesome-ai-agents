{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70684a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model= \"claude-3-5-haiku-latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd223744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\":\"user\", \"content\":text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "    \n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf5c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate an evaluation dataset for prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing a task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "- Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "- Focus on tasks that do not require writing much code.\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\".strip()\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4159d0",
   "metadata": {},
   "source": [
    "#### Building the Core Functions\n",
    "The evaluation pipeline consists of three main functions, each with a specific responsibility. Let's start with the simplest one - the function that handles individual prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f91bf",
   "metadata": {},
   "source": [
    "##### The run_prompt Function\n",
    "This function takes a test case and merges it with our prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case):\n",
    "     \"\"\"Merges the prompt and test case input, then returns the result\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\"\"\"\n",
    "messages = []\n",
    "add_user_message(messages, prompt)\n",
    "output = chat(messages)\n",
    "return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e579b27",
   "metadata": {},
   "source": [
    "Right now, we're keeping the prompt extremely simple. We're not including any formatting instructions, so Claude will likely return more verbose output than we need. We'll refine this later as we iterate on our prompt design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68478ed7",
   "metadata": {},
   "source": [
    "##### The run_test_case Function\n",
    "This function orchestrates running a single test case and grading the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b754f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case):\n",
    "    \"\"\"\n",
    "    calls  run_prompt and grades the result\n",
    "    \"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "    #todo - grading\n",
    "\n",
    "    score = 10\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f3b7a",
   "metadata": {},
   "source": [
    "For now, we're using a hardcoded score of 10. The grading logic is where we'll spend significant time in upcoming sections, but this placeholder lets us test the overall pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5559c",
   "metadata": {},
   "source": [
    "##### The run_eval Function\n",
    "This function coordinates the entire evaluation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b44b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataset):\n",
    "    \"\"\"\n",
    "    Loads the dataset and calls run_test_case with each case\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for test_case in dataset:\n",
    "        results.append(run_test_case(test_case))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d6901",
   "metadata": {},
   "source": [
    "#### This function processes every test case in our dataset and collects all the results into a single list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098dab6a",
   "metadata": {},
   "source": [
    "##### Running the Evaluation\n",
    "To execute our evaluation pipeline, we load our dataset and run it through our functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fce5a",
   "metadata": {},
   "source": [
    "The first time you run this, expect it to take some time - even with Claude Haiku, it can take around 30 seconds to process a full dataset. We'll cover optimization techniques later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80ea12",
   "metadata": {},
   "source": [
    "##### Examining the Results\n",
    "The evaluation returns a structured JSON array where each object represents one test case result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd4432",
   "metadata": {},
   "source": [
    "Each result contains three key pieces of information:\n",
    "\n",
    "* output: The complete response from Claude\n",
    "* test_case: The original test case that was processed\n",
    "* score: The evaluation score (currently hardcoded)\n",
    "As you can see in the output, Claude generates quite verbose responses since we haven't provided specific formatting instructions yet. This is exactly the kind of issue we'll address as we refine our prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103eb5e9",
   "metadata": {},
   "source": [
    "#### What We've Accomplished\n",
    "At this point, we've successfully built the core evaluation pipeline. We can take our dataset, process it through Claude, and collect structured results. The major missing piece is the grading system - that hardcoded score of 10 needs to be replaced with actual evaluation logic.\n",
    "\n",
    "This pipeline represents the foundation of most AI evaluation systems. While it may seem simple, you've just built the majority of what an eval pipeline actually does. The complexity comes in the details - better prompts, sophisticated grading, and performance optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a7222",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
