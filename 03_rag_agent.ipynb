{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) AI Agent\n",
    "\n",
    "This notebook demonstrates how to create a RAG-based AI agent that can retrieve information from external knowledge sources and generate informed responses.\n",
    "\n",
    "## Features:\n",
    "- Document ingestion and vector storage\n",
    "- Semantic search and retrieval\n",
    "- Context-aware response generation\n",
    "- Multiple document formats support\n",
    "- Relevance scoring and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai tiktoken numpy scikit-learn sentence-transformers python-dotenv PyPDF2 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document and Chunk Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a document in the knowledge base\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    source: str\n",
    "    created_at: datetime = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.created_at is None:\n",
    "            self.created_at = datetime.now()\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Represents a chunk of a document for retrieval\"\"\"\n",
    "    id: str\n",
    "    document_id: str\n",
    "    content: str\n",
    "    embedding: Optional[np.ndarray]\n",
    "    metadata: Dict[str, Any]\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "    \n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Represents a retrieval result with relevance score\"\"\"\n",
    "    chunk: DocumentChunk\n",
    "    score: float\n",
    "    document_title: str\n",
    "    document_source: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"Handles text processing and chunking for documents\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 encoding\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()\\[\\]-]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def split_text_by_tokens(self, text: str, max_tokens: int = None) -> List[str]:\n",
    "        \"\"\"Split text into chunks based on token count\"\"\"\n",
    "        if max_tokens is None:\n",
    "            max_tokens = self.chunk_size\n",
    "        \n",
    "        # Split by sentences first\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If adding this sentence would exceed token limit\n",
    "            if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = self._get_overlap_text(current_chunk, self.chunk_overlap)\n",
    "                current_chunk = overlap_text + sentence\n",
    "                current_tokens = self.count_tokens(current_chunk)\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _get_overlap_text(self, text: str, overlap_tokens: int) -> str:\n",
    "        \"\"\"Get the last portion of text for overlap\"\"\"\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return \"\"\n",
    "        \n",
    "        # Estimate words needed for overlap (rough approximation)\n",
    "        overlap_words = overlap_tokens // 4  # Assume ~4 tokens per word\n",
    "        overlap_words = min(overlap_words, len(words))\n",
    "        \n",
    "        return \" \".join(words[-overlap_words:]) + \" \"\n",
    "    \n",
    "    def create_chunks(self, document: Document) -> List[DocumentChunk]:\n",
    "        \"\"\"Create chunks from a document\"\"\"\n",
    "        cleaned_text = self.clean_text(document.content)\n",
    "        text_chunks = self.split_text_by_tokens(cleaned_text)\n",
    "        \n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            chunk_id = f\"{document.id}_chunk_{i}\"\n",
    "            end_index = start_index + len(chunk_text)\n",
    "            \n",
    "            chunk = DocumentChunk(\n",
    "                id=chunk_id,\n",
    "                document_id=document.id,\n",
    "                content=chunk_text,\n",
    "                embedding=None,  # Will be set later\n",
    "                metadata={\n",
    "                    **document.metadata,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"token_count\": self.count_tokens(chunk_text)\n",
    "                },\n",
    "                start_index=start_index,\n",
    "                end_index=end_index\n",
    "            )\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            start_index = end_index\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Simple in-memory vector store for document embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.chunks: Dict[str, DocumentChunk] = {}\n",
    "        self.documents: Dict[str, Document] = {}\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.chunk_ids: List[str] = []\n",
    "    \n",
    "    def add_document(self, document: Document, chunks: List[DocumentChunk]):\n",
    "        \"\"\"Add a document and its chunks to the vector store\"\"\"\n",
    "        self.documents[document.id] = document\n",
    "        \n",
    "        # Generate embeddings for chunks\n",
    "        chunk_texts = [chunk.content for chunk in chunks]\n",
    "        chunk_embeddings = self.embedding_model.encode(chunk_texts)\n",
    "        \n",
    "        # Store chunks with embeddings\n",
    "        for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "            chunk.embedding = embedding\n",
    "            self.chunks[chunk.id] = chunk\n",
    "        \n",
    "        # Update the embedding matrix\n",
    "        self._rebuild_embedding_matrix()\n",
    "    \n",
    "    def _rebuild_embedding_matrix(self):\n",
    "        \"\"\"Rebuild the embedding matrix for efficient similarity search\"\"\"\n",
    "        if not self.chunks:\n",
    "            self.embeddings = None\n",
    "            self.chunk_ids = []\n",
    "            return\n",
    "        \n",
    "        self.chunk_ids = list(self.chunks.keys())\n",
    "        embeddings_list = [self.chunks[chunk_id].embedding for chunk_id in self.chunk_ids]\n",
    "        self.embeddings = np.vstack(embeddings_list)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5, min_score: float = 0.0) -> List[RetrievalResult]:\n",
    "        \"\"\"Search for relevant chunks based on query\"\"\"\n",
    "        if not self.chunks or self.embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            score = similarities[idx]\n",
    "            if score < min_score:\n",
    "                break\n",
    "                \n",
    "            chunk_id = self.chunk_ids[idx]\n",
    "            chunk = self.chunks[chunk_id]\n",
    "            document = self.documents[chunk.document_id]\n",
    "            \n",
    "            result = RetrievalResult(\n",
    "                chunk=chunk,\n",
    "                score=score,\n",
    "                document_title=document.title,\n",
    "                document_source=document.source\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_document_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the vector store\"\"\"\n",
    "        return {\n",
    "            \"total_documents\": len(self.documents),\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"embedding_dimension\": self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            \"model_name\": self.embedding_model.get_sentence_embedding_dimension()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent:\n",
    "    \"\"\"Retrieval-Augmented Generation AI Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", max_context_tokens: int = 3000):\n",
    "        self.model = model\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.text_processor = TextProcessor()\n",
    "        self.vector_store = VectorStore()\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a knowledgeable AI assistant that uses retrieved information to provide accurate, \n",
    "        well-informed responses. When answering questions:\n",
    "        \n",
    "        1. Use the provided context information to support your answers\n",
    "        2. Clearly distinguish between information from the context and your general knowledge\n",
    "        3. If the context doesn't contain enough information, say so honestly\n",
    "        4. Cite sources when possible\n",
    "        5. Provide comprehensive but concise answers\n",
    "        \n",
    "        Always prioritize accuracy and helpfulness in your responses.\n",
    "        \"\"\"\n",
    "    \n",
    "    def add_document_from_text(self, title: str, content: str, source: str = \"manual\", metadata: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Add a document from text content\"\"\"\n",
    "        doc_id = f\"doc_{len(self.vector_store.documents)}_{hash(title) % 10000}\"\n",
    "        \n",
    "        document = Document(\n",
    "            id=doc_id,\n",
    "            title=title,\n",
    "            content=content,\n",
    "            metadata=metadata or {},\n",
    "            source=source\n",
    "        )\n",
    "        \n",
    "        chunks = self.text_processor.create_chunks(document)\n",
    "        self.vector_store.add_document(document, chunks)\n",
    "        \n",
    "        return doc_id\n",
    "    \n",
    "    def add_document_from_url(self, url: str, title: str = None) -> str:\n",
    "        \"\"\"Add a document by fetching content from a URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.text\n",
    "            # Basic HTML cleaning (you might want to use BeautifulSoup for better results)\n",
    "            content = re.sub(r'<[^>]+>', '', content)\n",
    "            content = re.sub(r'\\s+', ' ', content)\n",
    "            \n",
    "            if not title:\n",
    "                title = f\"Web content from {url}\"\n",
    "            \n",
    "            return self.add_document_from_text(title, content, source=url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to fetch content from {url}: {str(e)}\")\n",
    "    \n",
    "    def retrieve_context(self, query: str, top_k: int = 3, min_score: float = 0.1) -> Tuple[str, List[RetrievalResult]]:\n",
    "        \"\"\"Retrieve relevant context for a query\"\"\"\n",
    "        results = self.vector_store.search(query, top_k=top_k, min_score=min_score)\n",
    "        \n",
    "        if not results:\n",
    "            return \"\", []\n",
    "        \n",
    "        # Build context string\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            context_parts.append(\n",
    "                f\"[Source {i}: {result.document_title} (Score: {result.score:.3f})]\\n\"\n",
    "                f\"{result.chunk.content}\\n\"\n",
    "            )\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Ensure context doesn't exceed token limit\n",
    "        if self.text_processor.count_tokens(context) > self.max_context_tokens:\n",
    "            # Truncate context to fit within limits\n",
    "            words = context.split()\n",
    "            truncated_words = words[:self.max_context_tokens]\n",
    "            context = \" \".join(truncated_words)\n",
    "        \n",
    "        return context, results\n",
    "    \n",
    "    def generate_response(self, query: str, use_conversation_history: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a response using RAG\"\"\"\n",
    "        # Retrieve relevant context\n",
    "        context, retrieval_results = self.retrieve_context(query)\n",
    "        \n",
    "        # Build messages for the API call\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "        # Add conversation history if requested\n",
    "        if use_conversation_history and self.conversation_history:\n",
    "            messages.extend(self.conversation_history[-6:])  # Last 3 exchanges\n",
    "        \n",
    "        # Add context and query\n",
    "        if context:\n",
    "            user_message = f\"\"\"\n",
    "            Context Information:\n",
    "            {context}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Please answer the question using the provided context information. \n",
    "            If the context doesn't contain sufficient information, please indicate that.\n",
    "            \"\"\"\n",
    "        else:\n",
    "            user_message = f\"\"\"\n",
    "            Question: {query}\n",
    "            \n",
    "            Note: No relevant context was found in the knowledge base. \n",
    "            Please answer based on your general knowledge but indicate that \n",
    "            this is not from the provided documents.\n",
    "            \"\"\"\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                max_tokens=500,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            # Update conversation history\n",
    "            if use_conversation_history:\n",
    "                self.conversation_history.extend([\n",
    "                    {\"role\": \"user\", \"content\": query},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "                ])\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": assistant_response,\n",
    "                \"context_used\": context,\n",
    "                \"retrieval_results\": retrieval_results,\n",
    "                \"sources_count\": len(retrieval_results)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"context_used\": context,\n",
    "                \"retrieval_results\": retrieval_results\n",
    "            }\n",
    "    \n",
    "    def clear_conversation_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def get_knowledge_base_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the knowledge base\"\"\"\n",
    "        stats = self.vector_store.get_document_stats()\n",
    "        stats[\"text_processor_settings\"] = {\n",
    "            \"chunk_size\": self.text_processor.chunk_size,\n",
    "            \"chunk_overlap\": self.text_processor.chunk_overlap\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def list_documents(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all documents in the knowledge base\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"id\": doc.id,\n",
    "                \"title\": doc.title,\n",
    "                \"source\": doc.source,\n",
    "                \"created_at\": doc.created_at.isoformat(),\n",
    "                \"content_length\": len(doc.content),\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            for doc in self.vector_store.documents.values()\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage and Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG agent\n",
    "rag_agent = RAGAgent()\n",
    "\n",
    "print(\"=== RAG Agent Initialized ===\")\n",
    "print(f\"Model: {rag_agent.model}\")\n",
    "print(f\"Max context tokens: {rag_agent.max_context_tokens}\")\n",
    "print(f\"Chunk size: {rag_agent.text_processor.chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sample documents to demonstrate RAG capabilities\n",
    "\n",
    "# Document 1: AI and Machine Learning overview\n",
    "ai_content = \"\"\"\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines \n",
    "that can perform tasks that typically require human intelligence. These tasks include learning, \n",
    "reasoning, problem-solving, perception, and language understanding.\n",
    "\n",
    "Machine Learning (ML) is a subset of AI that focuses on the development of algorithms and statistical \n",
    "models that enable computers to improve their performance on a specific task through experience, \n",
    "without being explicitly programmed.\n",
    "\n",
    "Deep Learning is a subset of machine learning that uses artificial neural networks with multiple \n",
    "layers (hence \"deep\") to model and understand complex patterns in data. It has been particularly \n",
    "successful in areas such as image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "Common applications of AI include:\n",
    "- Computer vision and image recognition\n",
    "- Natural language processing and chatbots\n",
    "- Recommendation systems\n",
    "- Autonomous vehicles\n",
    "- Predictive analytics\n",
    "- Game playing (like chess and Go)\n",
    "\n",
    "The field of AI has experienced rapid growth in recent years, driven by advances in computing power, \n",
    "the availability of large datasets, and improvements in algorithms.\n",
    "\"\"\"\n",
    "\n",
    "doc1_id = rag_agent.add_document_from_text(\n",
    "    title=\"Introduction to Artificial Intelligence and Machine Learning\",\n",
    "    content=ai_content,\n",
    "    source=\"AI Tutorial\",\n",
    "    metadata={\"category\": \"education\", \"topic\": \"AI/ML\"}\n",
    ")\n",
    "\n",
    "# Document 2: Python programming guide\n",
    "python_content = \"\"\"\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
    "It was created by Guido van Rossum and first released in 1991.\n",
    "\n",
    "Key features of Python:\n",
    "- Easy to learn and use syntax\n",
    "- Interpreted language (no compilation needed)\n",
    "- Object-oriented and functional programming support\n",
    "- Extensive standard library\n",
    "- Large ecosystem of third-party packages\n",
    "- Cross-platform compatibility\n",
    "\n",
    "Python is widely used in various domains:\n",
    "- Web development (Django, Flask)\n",
    "- Data science and analytics (Pandas, NumPy, Matplotlib)\n",
    "- Machine learning (Scikit-learn, TensorFlow, PyTorch)\n",
    "- Automation and scripting\n",
    "- Scientific computing\n",
    "- Game development\n",
    "\n",
    "Popular Python libraries:\n",
    "- NumPy: Numerical computing\n",
    "- Pandas: Data manipulation and analysis\n",
    "- Matplotlib: Data visualization\n",
    "- Requests: HTTP library\n",
    "- Django: Web framework\n",
    "- Flask: Lightweight web framework\n",
    "\n",
    "Python's philosophy emphasizes code readability and simplicity, following the principle that \n",
    "\"there should be one obvious way to do it.\"\n",
    "\"\"\"\n",
    "\n",
    "doc2_id = rag_agent.add_document_from_text(\n",
    "    title=\"Python Programming Language Guide\",\n",
    "    content=python_content,\n",
    "    source=\"Programming Tutorial\",\n",
    "    metadata={\"category\": \"programming\", \"language\": \"python\"}\n",
    ")\n",
    "\n",
    "# Document 3: Data Science overview\n",
    "data_science_content = \"\"\"\n",
    "Data Science is an interdisciplinary field that combines statistics, computer science, and domain \n",
    "expertise to extract insights and knowledge from data. It involves collecting, processing, analyzing, \n",
    "and interpreting large amounts of data to inform decision-making.\n",
    "\n",
    "The data science process typically includes:\n",
    "1. Problem definition and goal setting\n",
    "2. Data collection and acquisition\n",
    "3. Data cleaning and preprocessing\n",
    "4. Exploratory data analysis (EDA)\n",
    "5. Feature engineering and selection\n",
    "6. Model building and training\n",
    "7. Model evaluation and validation\n",
    "8. Deployment and monitoring\n",
    "\n",
    "Key skills for data scientists:\n",
    "- Programming (Python, R, SQL)\n",
    "- Statistics and probability\n",
    "- Machine learning algorithms\n",
    "- Data visualization\n",
    "- Domain knowledge\n",
    "- Communication skills\n",
    "\n",
    "Common tools and technologies:\n",
    "- Python libraries: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn\n",
    "- R and RStudio\n",
    "- SQL databases\n",
    "- Jupyter Notebooks\n",
    "- Tableau, Power BI for visualization\n",
    "- Apache Spark for big data processing\n",
    "- Cloud platforms: AWS, Google Cloud, Azure\n",
    "\n",
    "Data science applications span many industries including healthcare, finance, retail, technology, \n",
    "and government, helping organizations make data-driven decisions.\n",
    "\"\"\"\n",
    "\n",
    "doc3_id = rag_agent.add_document_from_text(\n",
    "    title=\"Data Science: Process, Skills, and Tools\",\n",
    "    content=data_science_content,\n",
    "    source=\"Data Science Guide\",\n",
    "    metadata={\"category\": \"data science\", \"topic\": \"overview\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Documents Added ===\")\n",
    "print(f\"Document 1 ID: {doc1_id}\")\n",
    "print(f\"Document 2 ID: {doc2_id}\")\n",
    "print(f\"Document 3 ID: {doc3_id}\")\n",
    "\n",
    "# Show knowledge base stats\n",
    "stats = rag_agent.get_knowledge_base_stats()\n",
    "print(f\"\\nKnowledge Base Stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic RAG Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Example 1: Basic RAG Queries ===\")\n",
    "\n",
    "# Query 1: About AI and ML\n",
    "query1 = \"What is machine learning and how does it relate to AI?\"\n",
    "result1 = rag_agent.generate_response(query1)\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(f\"Response: {result1.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {result1.get('sources_count', 0)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Query 2: About Python\n",
    "query2 = \"What are the key features of Python programming language?\"\n",
    "result2 = rag_agent.generate_response(query2)\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(f\"Response: {result2.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {result2.get('sources_count', 0)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Query 3: About Data Science\n",
    "query3 = \"What skills do I need to become a data scientist?\"\n",
    "result3 = rag_agent.generate_response(query3)\n",
    "\n",
    "print(f\"Query: {query3}\")\n",
    "print(f\"Response: {result3.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {result3.get('sources_count', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Cross-Document Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Example 2: Cross-Document Queries ===\")\n",
    "\n",
    "# Query that should pull information from multiple documents\n",
    "cross_query = \"How is Python used in machine learning and data science?\"\n",
    "cross_result = rag_agent.generate_response(cross_query)\n",
    "\n",
    "print(f\"Query: {cross_query}\")\n",
    "print(f\"Response: {cross_result.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {cross_result.get('sources_count', 0)}\")\n",
    "\n",
    "# Show which sources were used\n",
    "if cross_result.get('retrieval_results'):\n",
    "    print(\"\\nRetrieved sources:\")\n",
    "    for i, result in enumerate(cross_result['retrieval_results'], 1):\n",
    "        print(f\"{i}. {result.document_title} (Score: {result.score:.3f})\")\n",
    "        print(f\"   Content preview: {result.chunk.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Conversation with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Example 3: Conversation with Context ===\")\n",
    "\n",
    "# Clear previous conversation history\n",
    "rag_agent.clear_conversation_history()\n",
    "\n",
    "# First question\n",
    "conv_query1 = \"Tell me about deep learning\"\n",
    "conv_result1 = rag_agent.generate_response(conv_query1)\n",
    "print(f\"User: {conv_query1}\")\n",
    "print(f\"Agent: {conv_result1.get('response', 'Error occurred')}\")\n",
    "print()\n",
    "\n",
    "# Follow-up question (should use conversation context)\n",
    "conv_query2 = \"What are some practical applications of it?\"\n",
    "conv_result2 = rag_agent.generate_response(conv_query2)\n",
    "print(f\"User: {conv_query2}\")\n",
    "print(f\"Agent: {conv_result2.get('response', 'Error occurred')}\")\n",
    "print()\n",
    "\n",
    "# Third question building on context\n",
    "conv_query3 = \"Which Python libraries would be useful for this?\"\n",
    "conv_result3 = rag_agent.generate_response(conv_query3)\n",
    "print(f\"User: {conv_query3}\")\n",
    "print(f\"Agent: {conv_result3.get('response', 'Error occurred')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Query Without Relevant Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Example 4: Query Without Relevant Context ===\")\n",
    "\n",
    "# Query about something not in our knowledge base\n",
    "no_context_query = \"What is the weather like today in New York?\"\n",
    "no_context_result = rag_agent.generate_response(no_context_query)\n",
    "\n",
    "print(f\"Query: {no_context_query}\")\n",
    "print(f\"Response: {no_context_result.get('response', 'Error occurred')}\")\n",
    "print(f\"Sources used: {no_context_result.get('sources_count', 0)}\")\n",
    "\n",
    "# The agent should indicate that this information is not in the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for RAG Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retrieval_quality(rag_agent, query: str, expected_docs: List[str] = None):\n",
    "    \"\"\"\n",
    "    Analyze the quality of retrieval for a given query.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Retrieval Analysis for: '{query}' ===\")\n",
    "    \n",
    "    # Get retrieval results\n",
    "    context, results = rag_agent.retrieve_context(query, top_k=5)\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} chunks:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Document: {result.document_title}\")\n",
    "        print(f\"   Source: {result.document_source}\")\n",
    "        print(f\"   Similarity Score: {result.score:.4f}\")\n",
    "        print(f\"   Content: {result.chunk.content[:200]}...\")\n",
    "    \n",
    "    if expected_docs:\n",
    "        retrieved_docs = [result.document_title for result in results]\n",
    "        print(f\"\\nExpected documents: {expected_docs}\")\n",
    "        print(f\"Retrieved documents: {retrieved_docs}\")\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap = set(expected_docs) & set(retrieved_docs)\n",
    "        print(f\"Overlap: {list(overlap)}\")\n",
    "        print(f\"Precision: {len(overlap) / len(retrieved_docs) if retrieved_docs else 0:.2f}\")\n",
    "        print(f\"Recall: {len(overlap) / len(expected_docs) if expected_docs else 0:.2f}\")\n",
    "\n",
    "# Example retrieval analysis\n",
    "analyze_retrieval_quality(\n",
    "    rag_agent, \n",
    "    \"machine learning algorithms\",\n",
    "    expected_docs=[\"Introduction to Artificial Intelligence and Machine Learning\"]\n",
    ")\n",
    "\n",
    "analyze_retrieval_quality(\n",
    "    rag_agent,\n",
    "    \"Python libraries for data analysis\",\n",
    "    expected_docs=[\"Python Programming Language Guide\", \"Data Science: Process, Skills, and Tools\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive RAG Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_rag_session(rag_agent, max_questions=3):\n",
    "    \"\"\"\n",
    "    Start an interactive RAG session.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Interactive RAG Session ===\")\n",
    "    print(\"I can answer questions based on the documents in my knowledge base.\")\n",
    "    print(f\"Knowledge base contains {len(rag_agent.list_documents())} documents.\")\n",
    "    print(\"Type 'quit' to end the session, 'docs' to list documents, 'stats' for statistics.\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    question_count = 0\n",
    "    while question_count < max_questions:\n",
    "        try:\n",
    "            user_input = input(f\"\\nQuestion {question_count + 1}/{max_questions}: \")\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"Session ended. Thank you!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'docs':\n",
    "                print(\"\\nDocuments in knowledge base:\")\n",
    "                for doc in rag_agent.list_documents():\n",
    "                    print(f\"- {doc['title']} (Source: {doc['source']})\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'stats':\n",
    "                stats = rag_agent.get_knowledge_base_stats()\n",
    "                print(\"\\nKnowledge base statistics:\")\n",
    "                for key, value in stats.items():\n",
    "                    print(f\"- {key}: {value}\")\n",
    "                continue\n",
    "            \n",
    "            if not user_input.strip():\n",
    "                print(\"Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            # Generate response\n",
    "            result = rag_agent.generate_response(user_input)\n",
    "            \n",
    "            if result.get('success'):\n",
    "                print(f\"\\nAnswer: {result['response']}\")\n",
    "                if result.get('sources_count', 0) > 0:\n",
    "                    print(f\"\\n[Based on {result['sources_count']} source(s) from the knowledge base]\")\n",
    "                else:\n",
    "                    print(f\"\\n[No relevant sources found in knowledge base - using general knowledge]\")\n",
    "            else:\n",
    "                print(f\"\\nError: {result.get('error', 'Unknown error occurred')}\")\n",
    "            \n",
    "            question_count += 1\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nSession interrupted.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Uncomment to run interactive session\n",
    "# interactive_rag_session(rag_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_knowledge_base_info(rag_agent):\n",
    "    \"\"\"\n",
    "    Display comprehensive information about the knowledge base.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Knowledge Base Information ===\")\n",
    "    \n",
    "    # Basic stats\n",
    "    stats = rag_agent.get_knowledge_base_stats()\n",
    "    print(f\"Total documents: {stats['total_documents']}\")\n",
    "    print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"Embedding dimension: {stats['embedding_dimension']}\")\n",
    "    \n",
    "    # Document details\n",
    "    print(\"\\nDocument Details:\")\n",
    "    docs = rag_agent.list_documents()\n",
    "    for doc in docs:\n",
    "        print(f\"\\n- ID: {doc['id']}\")\n",
    "        print(f\"  Title: {doc['title']}\")\n",
    "        print(f\"  Source: {doc['source']}\")\n",
    "        print(f\"  Content length: {doc['content_length']} characters\")\n",
    "        print(f\"  Created: {doc['created_at']}\")\n",
    "        if doc['metadata']:\n",
    "            print(f\"  Metadata: {doc['metadata']}\")\n",
    "    \n",
    "    # Chunk distribution\n",
    "    chunks_per_doc = {}\n",
    "    for chunk_id, chunk in rag_agent.vector_store.chunks.items():\n",
    "        doc_id = chunk.document_id\n",
    "        if doc_id not in chunks_per_doc:\n",
    "            chunks_per_doc[doc_id] = 0\n",
    "        chunks_per_doc[doc_id] += 1\n",
    "    \n",
    "    print(\"\\nChunks per document:\")\n",
    "    for doc_id, chunk_count in chunks_per_doc.items():\n",
    "        doc_title = next((doc['title'] for doc in docs if doc['id'] == doc_id), doc_id)\n",
    "        print(f\"- {doc_title}: {chunk_count} chunks\")\n",
    "\n",
    "# Display current knowledge base info\n",
    "display_knowledge_base_info(rag_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG Features\n",
    "\n",
    "Here are some advanced features and extensions you can implement:\n",
    "\n",
    "### 1. Multi-Modal RAG\n",
    "Extend to handle images, audio, and video content alongside text.\n",
    "\n",
    "### 2. Hierarchical Document Structure\n",
    "Maintain document structure (headings, sections) for better context.\n",
    "\n",
    "### 3. Dynamic Knowledge Updates\n",
    "Automatically update the knowledge base from web sources or databases.\n",
    "\n",
    "### 4. Fact Verification\n",
    "Cross-reference information across multiple sources for accuracy.\n",
    "\n",
    "### 5. Personalized Retrieval\n",
    "Adapt retrieval based on user preferences and interaction history.\n",
    "\n",
    "### 6. Explainable AI\n",
    "Provide detailed explanations of why certain sources were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Enhanced RAG with confidence scoring\n",
    "class EnhancedRAGAgent(RAGAgent):\n",
    "    \"\"\"Enhanced RAG agent with additional features\"\"\"\n",
    "    \n",
    "    def generate_response_with_confidence(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response with confidence scoring\"\"\"\n",
    "        result = self.generate_response(query)\n",
    "        \n",
    "        if result.get('success') and result.get('retrieval_results'):\n",
    "            # Calculate confidence based on retrieval scores\n",
    "            scores = [r.score for r in result['retrieval_results']]\n",
    "            avg_score = np.mean(scores) if scores else 0\n",
    "            max_score = max(scores) if scores else 0\n",
    "            \n",
    "            # Simple confidence calculation\n",
    "            confidence = min(1.0, (avg_score + max_score) / 2)\n",
    "            \n",
    "            # Confidence levels\n",
    "            if confidence >= 0.8:\n",
    "                confidence_level = \"High\"\n",
    "            elif confidence >= 0.6:\n",
    "                confidence_level = \"Medium\"\n",
    "            elif confidence >= 0.4:\n",
    "                confidence_level = \"Low\"\n",
    "            else:\n",
    "                confidence_level = \"Very Low\"\n",
    "            \n",
    "            result['confidence_score'] = confidence\n",
    "            result['confidence_level'] = confidence_level\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage of enhanced agent\n",
    "enhanced_agent = EnhancedRAGAgent()\n",
    "\n",
    "# Add the same documents\n",
    "enhanced_agent.add_document_from_text(\n",
    "    title=\"AI Overview\", \n",
    "    content=ai_content, \n",
    "    source=\"tutorial\"\n",
    ")\n",
    "\n",
    "# Test with confidence scoring\n",
    "enhanced_result = enhanced_agent.generate_response_with_confidence(\n",
    "    \"What is artificial intelligence?\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Enhanced RAG with Confidence Scoring ===\")\n",
    "if enhanced_result.get('success'):\n",
    "    print(f\"Response: {enhanced_result['response']}\")\n",
    "    print(f\"Confidence Score: {enhanced_result.get('confidence_score', 'N/A'):.3f}\")\n",
    "    print(f\"Confidence Level: {enhanced_result.get('confidence_level', 'N/A')}\")\n",
    "    print(f\"Sources: {enhanced_result.get('sources_count', 0)}\")\n",
    "else:\n",
    "    print(f\"Error: {enhanced_result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This RAG AI agent demonstrates:\n",
    "\n",
    "1. **Document Ingestion**: Converting text documents into searchable chunks with embeddings\n",
    "2. **Semantic Search**: Finding relevant information using vector similarity\n",
    "3. **Context Integration**: Combining retrieved information with AI generation\n",
    "4. **Conversation Memory**: Maintaining context across multiple exchanges\n",
    "5. **Quality Assessment**: Evaluating retrieval quality and response confidence\n",
    "\n",
    "### Key Benefits of RAG:\n",
    "- **Factual Accuracy**: Responses grounded in provided documents\n",
    "- **Up-to-date Information**: Knowledge base can be updated without retraining\n",
    "- **Source Attribution**: Clear indication of information sources\n",
    "- **Domain Specialization**: Focus on specific knowledge domains\n",
    "- **Scalability**: Can handle large document collections efficiently\n",
    "\n",
    "### Use Cases:\n",
    "- Customer support systems\n",
    "- Internal knowledge management\n",
    "- Research assistants\n",
    "- Educational tutoring systems\n",
    "- Technical documentation helpers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}